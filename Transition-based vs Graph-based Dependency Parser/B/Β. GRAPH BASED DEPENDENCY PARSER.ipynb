{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment #4\n",
    "### by Prodromos Kampouridis MTN2203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT NOTE\n",
    "##### *Due to the large length of code, the answers to tasks 1-5 can also be found as markdowns in the cells below.*\n",
    "\n",
    "\n",
    "##### *For more detailed information, please refer to the report entitled PRODROMOS KAMPOURIDIS REPORT*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnbFAjtPw0P8"
   },
   "source": [
    "## Î’. GRAPH-BASED DEPENDENCY PARSER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyIF88VcmRnN"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this part of the assignment we will explore a graph-based dependency parser based on the work of Kiperwasser and Goldberg (2016). The model is based on a bidirectional LSTM encoder and two MLPs. One for predicting a score for each possible dependency, and one to detect the type of the dependency. As part of the assignment, we will explore various modifications in the model's architecture, as well as the change of the BI-LSTM encoder with a BERT encoder, and the effect of these changes in the performance of the model, in terms of UAS and LAS evaluation metrics.\n",
    "\n",
    "We have modified the `model.py` and `main.py` files to implement the changes described in the questions. Moreover, we have created a new file `model_bert.py` to implement the BERT based model of question B4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqxJxub_w7X2"
   },
   "source": [
    "### 1.\n",
    "First, we run the experiment with 1 BiLSTM layer, by executing `main.py` with the argument `--n_lstm_layers 1`. To evaluate the model's performance in the test set, we then execute the `main.py` script with the `--do_eval` and `--model_dir` arguments. As the `model_dir`, we pass the path of the save model.\n",
    "\n",
    "Commands:\n",
    "```\n",
    "python main.py --n_lstm_layers 1\n",
    "python main.py --n_lstm_layers 1 --do_eval --model_dir \"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_date=06_20_2023\"\n",
    "```\n",
    "\n",
    "Results:\n",
    "\n",
    "The model's performance in the test set is:\n",
    "\n",
    "UAS: 93.56\n",
    "\n",
    "LAS: 92.05\n",
    "\n",
    "Compared to the best models of part A (A1 and A4) that both achieved UAS 89.19, this model performs significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z6UAJn5w5WN",
    "outputId": "c2954d5b-e83c-4c0c-d09c-f3efe98e9045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:01:35,116 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': None, 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': False, 'pretrained_emb': None, 'activation_function': 'tanh', 'encoder': 'lstm', 'experiment_dir': './results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_date=06_20_2023'}\n",
      "2023-06-20 14:01:37,360 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-20 14:01:57,827 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 14:01:57,827 - INFO - Train epoch: 1\n",
      "2023-06-20 14:01:58,951 - INFO - [0/39832 (0%)]\n",
      "2023-06-20 14:03:47,528 - INFO - [2000/39832 (5%)]\n",
      "2023-06-20 14:05:31,875 - INFO - [4000/39832 (10%)]\n",
      "2023-06-20 14:07:12,404 - INFO - [6000/39832 (15%)]\n",
      "2023-06-20 14:08:54,603 - INFO - [8000/39832 (20%)]\n",
      "2023-06-20 14:10:34,017 - INFO - [10000/39832 (25%)]\n",
      "2023-06-20 14:12:14,223 - INFO - [12000/39832 (30%)]\n",
      "2023-06-20 14:13:56,756 - INFO - [14000/39832 (35%)]\n",
      "2023-06-20 14:15:37,250 - INFO - [16000/39832 (40%)]\n",
      "2023-06-20 14:17:17,167 - INFO - [18000/39832 (45%)]\n",
      "2023-06-20 14:18:57,260 - INFO - [20000/39832 (50%)]\n",
      "2023-06-20 14:20:38,589 - INFO - [22000/39832 (55%)]\n",
      "2023-06-20 14:22:19,654 - INFO - [24000/39832 (60%)]\n",
      "2023-06-20 14:23:56,912 - INFO - [26000/39832 (65%)]\n",
      "2023-06-20 14:25:38,150 - INFO - [28000/39832 (70%)]\n",
      "2023-06-20 14:27:18,956 - INFO - [30000/39832 (75%)]\n",
      "2023-06-20 14:29:00,028 - INFO - [32000/39832 (80%)]\n",
      "2023-06-20 14:30:38,963 - INFO - [34000/39832 (85%)]\n",
      "2023-06-20 14:32:18,502 - INFO - [36000/39832 (90%)]\n",
      "2023-06-20 14:33:59,639 - INFO - [38000/39832 (95%)]\n",
      "2023-06-20 14:36:00,557 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 14:36:00,558 - INFO - dev results:\n",
      "2023-06-20 14:36:00,558 - INFO - Metric              |Score |\n",
      "2023-06-20 14:36:00,558 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 14:36:00,558 - INFO - las                 | 90.78|\n",
      "2023-06-20 14:36:00,558 - INFO - uas                 | 92.69|\n",
      "2023-06-20 14:36:00,706 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-20 14:36:00,706 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 14:36:00,707 - INFO - Train epoch: 2\n",
      "2023-06-20 14:36:00,740 - INFO - [0/39832 (0%)]\n",
      "2023-06-20 14:37:39,935 - INFO - [2000/39832 (5%)]\n",
      "2023-06-20 14:39:17,953 - INFO - [4000/39832 (10%)]\n",
      "2023-06-20 14:40:53,934 - INFO - [6000/39832 (15%)]\n",
      "2023-06-20 14:42:30,733 - INFO - [8000/39832 (20%)]\n",
      "2023-06-20 14:44:06,438 - INFO - [10000/39832 (25%)]\n",
      "2023-06-20 14:45:44,003 - INFO - [12000/39832 (30%)]\n",
      "2023-06-20 14:47:19,606 - INFO - [14000/39832 (35%)]\n",
      "2023-06-20 14:48:56,194 - INFO - [16000/39832 (40%)]\n",
      "2023-06-20 14:50:34,032 - INFO - [18000/39832 (45%)]\n",
      "2023-06-20 14:52:09,505 - INFO - [20000/39832 (50%)]\n",
      "2023-06-20 14:53:43,535 - INFO - [22000/39832 (55%)]\n",
      "2023-06-20 14:55:22,096 - INFO - [24000/39832 (60%)]\n",
      "2023-06-20 14:56:58,226 - INFO - [26000/39832 (65%)]\n",
      "2023-06-20 14:58:31,979 - INFO - [28000/39832 (70%)]\n",
      "2023-06-20 15:00:09,049 - INFO - [30000/39832 (75%)]\n",
      "2023-06-20 15:01:46,410 - INFO - [32000/39832 (80%)]\n",
      "2023-06-20 15:03:22,339 - INFO - [34000/39832 (85%)]\n",
      "2023-06-20 15:04:56,775 - INFO - [36000/39832 (90%)]\n",
      "2023-06-20 15:06:33,982 - INFO - [38000/39832 (95%)]\n",
      "2023-06-20 15:08:30,500 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:08:30,501 - INFO - dev results:\n",
      "2023-06-20 15:08:30,501 - INFO - Metric              |Score |\n",
      "2023-06-20 15:08:30,501 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:08:30,501 - INFO - las                 | 91.73|\n",
      "2023-06-20 15:08:30,501 - INFO - uas                 | 93.40|\n",
      "2023-06-20 15:08:30,715 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-20 15:08:30,718 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:08:30,718 - INFO - Train epoch: 3\n",
      "2023-06-20 15:08:30,832 - INFO - [0/39832 (0%)]\n",
      "2023-06-20 15:10:05,751 - INFO - [2000/39832 (5%)]\n",
      "2023-06-20 15:11:39,862 - INFO - [4000/39832 (10%)]\n",
      "2023-06-20 15:13:16,983 - INFO - [6000/39832 (15%)]\n",
      "2023-06-20 15:14:52,379 - INFO - [8000/39832 (20%)]\n",
      "2023-06-20 15:16:28,624 - INFO - [10000/39832 (25%)]\n",
      "2023-06-20 15:18:02,477 - INFO - [12000/39832 (30%)]\n",
      "2023-06-20 15:19:35,286 - INFO - [14000/39832 (35%)]\n",
      "2023-06-20 15:21:09,324 - INFO - [16000/39832 (40%)]\n",
      "2023-06-20 15:22:43,444 - INFO - [18000/39832 (45%)]\n",
      "2023-06-20 15:24:18,084 - INFO - [20000/39832 (50%)]\n",
      "2023-06-20 15:25:54,352 - INFO - [22000/39832 (55%)]\n",
      "2023-06-20 15:27:28,109 - INFO - [24000/39832 (60%)]\n",
      "2023-06-20 15:29:04,902 - INFO - [26000/39832 (65%)]\n",
      "2023-06-20 15:30:41,627 - INFO - [28000/39832 (70%)]\n",
      "2023-06-20 15:32:16,146 - INFO - [30000/39832 (75%)]\n",
      "2023-06-20 15:33:49,593 - INFO - [32000/39832 (80%)]\n",
      "2023-06-20 15:35:25,494 - INFO - [34000/39832 (85%)]\n",
      "2023-06-20 15:36:59,779 - INFO - [36000/39832 (90%)]\n",
      "2023-06-20 15:38:35,831 - INFO - [38000/39832 (95%)]\n",
      "2023-06-20 15:40:32,200 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:40:32,200 - INFO - dev results:\n",
      "2023-06-20 15:40:32,201 - INFO - Metric              |Score |\n",
      "2023-06-20 15:40:32,201 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:40:32,201 - INFO - las                 | 91.82|\n",
      "2023-06-20 15:40:32,201 - INFO - uas                 | 93.58|\n",
      "2023-06-20 15:40:32,338 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-20 15:40:32,341 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 15:40:32,341 - INFO - Train epoch: 4\n",
      "2023-06-20 15:40:32,393 - INFO - [0/39832 (0%)]\n",
      "2023-06-20 15:42:07,910 - INFO - [2000/39832 (5%)]\n",
      "2023-06-20 15:43:42,818 - INFO - [4000/39832 (10%)]\n",
      "2023-06-20 15:45:16,067 - INFO - [6000/39832 (15%)]\n",
      "2023-06-20 15:46:49,738 - INFO - [8000/39832 (20%)]\n",
      "2023-06-20 15:48:25,268 - INFO - [10000/39832 (25%)]\n",
      "2023-06-20 15:50:00,971 - INFO - [12000/39832 (30%)]\n",
      "2023-06-20 15:51:36,297 - INFO - [14000/39832 (35%)]\n",
      "2023-06-20 15:53:09,357 - INFO - [16000/39832 (40%)]\n",
      "2023-06-20 15:54:43,611 - INFO - [18000/39832 (45%)]\n",
      "2023-06-20 15:56:19,360 - INFO - [20000/39832 (50%)]\n",
      "2023-06-20 15:57:54,806 - INFO - [22000/39832 (55%)]\n",
      "2023-06-20 15:59:27,156 - INFO - [24000/39832 (60%)]\n",
      "2023-06-20 16:01:01,435 - INFO - [26000/39832 (65%)]\n",
      "2023-06-20 16:02:36,825 - INFO - [28000/39832 (70%)]\n",
      "2023-06-20 16:04:11,177 - INFO - [30000/39832 (75%)]\n",
      "2023-06-20 16:05:45,257 - INFO - [32000/39832 (80%)]\n",
      "2023-06-20 16:07:19,053 - INFO - [34000/39832 (85%)]\n",
      "2023-06-20 16:08:54,229 - INFO - [36000/39832 (90%)]\n",
      "2023-06-20 16:10:28,674 - INFO - [38000/39832 (95%)]\n",
      "2023-06-20 16:12:26,885 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 16:12:26,886 - INFO - dev results:\n",
      "2023-06-20 16:12:26,886 - INFO - Metric              |Score |\n",
      "2023-06-20 16:12:26,886 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 16:12:26,886 - INFO - las                 | 91.89|\n",
      "2023-06-20 16:12:26,886 - INFO - uas                 | 93.51|\n",
      "2023-06-20 16:12:26,983 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 16:12:26,984 - INFO - Train epoch: 5\n",
      "2023-06-20 16:12:27,026 - INFO - [0/39832 (0%)]\n",
      "2023-06-20 16:14:01,337 - INFO - [2000/39832 (5%)]\n",
      "2023-06-20 16:15:33,354 - INFO - [4000/39832 (10%)]\n",
      "2023-06-20 16:17:07,854 - INFO - [6000/39832 (15%)]\n",
      "2023-06-20 16:18:42,784 - INFO - [8000/39832 (20%)]\n",
      "2023-06-20 16:20:16,795 - INFO - [10000/39832 (25%)]\n",
      "2023-06-20 16:21:51,636 - INFO - [12000/39832 (30%)]\n",
      "2023-06-20 16:23:26,334 - INFO - [14000/39832 (35%)]\n",
      "2023-06-20 16:25:00,653 - INFO - [16000/39832 (40%)]\n",
      "2023-06-20 16:26:37,267 - INFO - [18000/39832 (45%)]\n",
      "2023-06-20 16:28:13,590 - INFO - [20000/39832 (50%)]\n",
      "2023-06-20 16:29:49,091 - INFO - [22000/39832 (55%)]\n",
      "2023-06-20 16:31:23,552 - INFO - [24000/39832 (60%)]\n",
      "2023-06-20 16:32:59,049 - INFO - [26000/39832 (65%)]\n",
      "2023-06-20 16:34:33,627 - INFO - [28000/39832 (70%)]\n",
      "2023-06-20 16:36:08,547 - INFO - [30000/39832 (75%)]\n",
      "2023-06-20 16:37:43,373 - INFO - [32000/39832 (80%)]\n",
      "2023-06-20 16:39:18,711 - INFO - [34000/39832 (85%)]\n",
      "2023-06-20 16:40:54,784 - INFO - [36000/39832 (90%)]\n",
      "2023-06-20 16:42:29,097 - INFO - [38000/39832 (95%)]\n",
      "2023-06-20 16:44:24,320 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 16:44:24,321 - INFO - dev results:\n",
      "2023-06-20 16:44:24,321 - INFO - Metric              |Score |\n",
      "2023-06-20 16:44:24,321 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-20 16:44:24,321 - INFO - las                 | 91.82|\n",
      "2023-06-20 16:44:24,321 - INFO - uas                 | 93.42|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoiLyCAd7hXg",
    "outputId": "8fcabe97-23c6-4816-d33f-70e027bd6357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 21:14:41,626 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_date=06_20_2023', 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': True, 'pretrained_emb': None, 'activation_function': 'tanh', 'encoder': 'lstm', 'experiment_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_date=06_20_2023'}\n",
      "2023-06-21 21:14:41,938 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-21 21:15:30,711 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 21:15:30,711 - INFO - test results:\n",
      "2023-06-21 21:15:30,712 - INFO - Metric              |Score |\n",
      "2023-06-21 21:15:30,712 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 21:15:30,712 - INFO - las                 | 92.05|\n",
      "2023-06-21 21:15:30,712 - INFO - uas                 | 93.56|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1 --do_eval --model_dir \"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_date=06_20_2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTuR9LUi_bKA"
   },
   "source": [
    "### 2.\n",
    "In order to replace the randomly initialized embeddings with pre-trained embeddings, we first added an extra command line argument in `main.py` named `--pretrained_embed`. Note that `main.py` has already available the `--ext_emb` argument, which however does not replace the randomly initialized embeddings with the ones in the given file, but rather concatenates the given embeddings along with the randomly initialized ones and the POS embeddings. We modified `main.py` so that when the `--pretrained_embed` is used, the `load_pretrained_word_embed` util function is called with input the given pre-trained embeddings file `glove.6B.100d.txt` and the already calculates word-index mapping. We use the returned w2i value of the function to replace the previous w2i variable, and thus restrict the vocabulary to contain the words with available embeddings, as follows.\n",
    "\n",
    "```\n",
    "pretrained_word_vectors = None\n",
    "if args.pretrained_emb is not None:\n",
    "    w2i, pretrained_word_vectors = load_pretrained_word_embed(args.pretrained_emb, w2i)\n",
    "```\n",
    "Then, we create the BISTParser with an extra parameter `pretrained_word_vectors` that contains the embeddings matrix. Finally, we modified the BISTParser so that when the `pretrained_word_vectors` is passed, the word_embedding layer is initialized with the pretrained embeddings rather than random ones:\n",
    "\n",
    "```\n",
    " # embedding layers initialization\n",
    "if pretrained_word_vectors is None:\n",
    "    self.word_embedding = nn.Embedding(len(w2i), w_emb_dim)\n",
    "else:\n",
    "    self.word_embedding = nn.Embedding.from_pretrained(pretrained_word_vectors, freeze=False)\n",
    "```\n",
    "\n",
    "Commands:\n",
    "```\n",
    "!python main.py --n_lstm_layers 1 --pretrained_emb ./glove.6B.100d.txt\n",
    "!python main.py --n_lstm_layers 1 --pretrained_emb ./glove.6B.100d.txt --do_eval --model_dir \"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=True_activation=tanh_encoder=lstm_date=06_21_2023\"\n",
    "```\n",
    "\n",
    "The model's performance in the test set is:\n",
    "\n",
    "UAS: 93.71\n",
    "\n",
    "LAS: 92.22\n",
    "\n",
    "Again this model outperforms the models of part A that achieved UAS: 89.19, by an even larger margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhB86yLBxGUm",
    "outputId": "d794a5c0-4bb6-45b5-aaa5-6e02d68a1e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 11:53:18,451 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': None, 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': False, 'pretrained_emb': './glove.6B.100d.txt', 'activation_function': 'tanh', 'encoder': 'lstm', 'experiment_dir': './results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=True_activation=tanh_encoder=lstm_date=06_21_2023'}\n",
      "2023-06-21 11:53:19,933 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-21 11:53:45,107 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 11:53:45,107 - INFO - Train epoch: 1\n",
      "2023-06-21 11:53:45,554 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 11:55:36,589 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 11:57:20,037 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 11:59:02,171 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 12:00:44,995 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 12:02:30,207 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 12:04:14,789 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 12:05:58,934 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 12:07:42,960 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 12:09:29,409 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 12:11:14,488 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 12:13:00,260 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 12:14:43,809 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 12:16:29,273 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 12:18:12,835 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 12:19:53,830 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 12:21:37,933 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 12:23:22,296 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 12:25:04,037 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 12:26:49,520 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 12:28:58,237 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 12:28:58,237 - INFO - dev results:\n",
      "2023-06-21 12:28:58,238 - INFO - Metric              |Score |\n",
      "2023-06-21 12:28:58,238 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 12:28:58,238 - INFO - las                 | 90.85|\n",
      "2023-06-21 12:28:58,238 - INFO - uas                 | 92.68|\n",
      "2023-06-21 12:28:58,509 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 12:28:58,509 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 12:28:58,510 - INFO - Train epoch: 2\n",
      "2023-06-21 12:28:58,561 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 12:30:41,181 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 12:32:23,765 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 12:34:08,669 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 12:35:51,902 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 12:37:34,582 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 12:39:14,428 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 12:40:55,605 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 12:42:40,299 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 12:44:20,505 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 12:46:02,244 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 12:47:45,713 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 12:49:28,650 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 12:51:12,629 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 12:52:54,331 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 12:54:34,268 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 12:56:14,446 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 12:57:56,865 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 12:59:38,371 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 13:01:19,871 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 13:03:26,207 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:03:26,207 - INFO - dev results:\n",
      "2023-06-21 13:03:26,207 - INFO - Metric              |Score |\n",
      "2023-06-21 13:03:26,207 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:03:26,207 - INFO - las                 | 91.40|\n",
      "2023-06-21 13:03:26,208 - INFO - uas                 | 93.12|\n",
      "2023-06-21 13:03:26,475 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 13:03:26,476 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:03:26,477 - INFO - Train epoch: 3\n",
      "2023-06-21 13:03:26,549 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 13:05:07,076 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 13:06:46,965 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 13:08:25,409 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 13:10:04,777 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 13:11:47,371 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 13:13:27,743 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 13:15:09,079 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 13:16:49,666 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 13:18:30,883 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 13:20:11,566 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 13:21:55,097 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 13:23:37,039 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 13:25:19,456 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 13:26:58,183 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 13:28:38,305 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 13:30:14,873 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 13:31:57,051 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 13:33:38,003 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 13:35:19,097 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 13:37:21,696 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:37:21,696 - INFO - dev results:\n",
      "2023-06-21 13:37:21,696 - INFO - Metric              |Score |\n",
      "2023-06-21 13:37:21,696 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:37:21,697 - INFO - las                 | 91.91|\n",
      "2023-06-21 13:37:21,697 - INFO - uas                 | 93.63|\n",
      "2023-06-21 13:37:21,858 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 13:37:21,861 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 13:37:21,861 - INFO - Train epoch: 4\n",
      "2023-06-21 13:37:21,913 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 13:39:02,413 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 13:40:40,603 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 13:42:21,356 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 13:43:58,285 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 13:45:37,097 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 13:47:13,139 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 13:48:52,807 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 13:50:31,402 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 13:52:16,266 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 13:54:01,446 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 13:55:45,909 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 13:57:31,673 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 13:59:12,197 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 14:00:56,917 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 14:02:41,560 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 14:04:25,302 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 14:06:12,087 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 14:07:56,425 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 14:09:40,588 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 14:11:49,915 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 14:11:49,915 - INFO - dev results:\n",
      "2023-06-21 14:11:49,915 - INFO - Metric              |Score |\n",
      "2023-06-21 14:11:49,915 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 14:11:49,916 - INFO - las                 | 91.78|\n",
      "2023-06-21 14:11:49,916 - INFO - uas                 | 93.37|\n",
      "2023-06-21 14:11:50,007 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 14:11:50,007 - INFO - Train epoch: 5\n",
      "2023-06-21 14:11:50,105 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 14:13:35,200 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 14:15:13,734 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 14:16:57,175 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 14:18:39,424 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 14:20:20,354 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 14:22:00,865 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 14:23:43,810 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 14:25:24,848 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 14:27:05,884 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 14:28:50,603 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 14:30:31,396 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 14:32:14,315 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 14:33:56,712 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 14:35:40,740 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 14:37:26,851 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 14:39:09,057 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 14:40:50,237 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 14:42:30,903 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 14:44:16,040 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 14:46:21,986 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 14:46:21,987 - INFO - dev results:\n",
      "2023-06-21 14:46:21,987 - INFO - Metric              |Score |\n",
      "2023-06-21 14:46:21,987 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 14:46:21,987 - INFO - las                 | 91.74|\n",
      "2023-06-21 14:46:21,987 - INFO - uas                 | 93.37|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1 --pretrained_emb ./glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ySxXQGbcGPc",
    "outputId": "a9ecd1f2-0110-4605-8235-deaeff41ce25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 20:55:11,316 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=True_activation=tanh_encoder=lstm_date=06_21_2023', 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': True, 'pretrained_emb': './glove.6B.100d.txt', 'activation_function': 'tanh', 'encoder': 'lstm', 'experiment_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=True_activation=tanh_encoder=lstm_date=06_21_2023'}\n",
      "2023-06-21 20:55:11,590 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-21 20:56:15,368 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:56:15,369 - INFO - test results:\n",
      "2023-06-21 20:56:15,369 - INFO - Metric              |Score |\n",
      "2023-06-21 20:56:15,369 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:56:15,369 - INFO - las                 | 92.22|\n",
      "2023-06-21 20:56:15,370 - INFO - uas                 | 93.71|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1 --pretrained_emb ./glove.6B.100d.txt --do_eval --model_dir \"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=True_activation=tanh_encoder=lstm_date=06_21_2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA1jLej9_dhQ"
   },
   "source": [
    "### 3.\n",
    "We modified the BISTParser class in the `model.py` file, to take `activation_function` as a parameter in the constructor. Then, we create the `self.slp_out_arc` and `self.slp_out_rel` layers with `nn.Tanh` if the name of the given activation function is `tanh`, or with `nn.ReLU` if the name is `relu`. We have also added an extra command line argument `activation_function` in the `main.py` file, which passes the given activation function name (default: tanh) to the BISTParser constructor during the parser initialization. To train and evaluate the model with ReLU activation function, we run the following commands:\n",
    "\n",
    "```\n",
    "python main.py --n_lstm_layers 1 --activation_function relu\n",
    "python main.py --n_lstm_layers 1 --activation_function relu --do_eval --model_dir=\"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=False_activation=relu_encoder=lstm_date=06_21_2023\"\n",
    "```\n",
    "\n",
    "The model's performance in the test set is:\n",
    "\n",
    "UAS: 93.66\n",
    "\n",
    "LAS: 92.16\n",
    "\n",
    "The model performs significantly better than the models of part A that achieved UAS: 89.19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDDetjptHzPf",
    "outputId": "d5220b6e-b300-468a-9011-14b6bb7d41dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 18:04:10,082 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': None, 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': False, 'pretrained_emb': None, 'activation_function': 'relu', 'encoder': 'lstm', 'experiment_dir': './results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=False_activation=relu_encoder=lstm_date=06_21_2023'}\n",
      "2023-06-21 18:04:12,971 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-21 18:04:26,383 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 18:04:26,383 - INFO - Train epoch: 1\n",
      "2023-06-21 18:04:26,852 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 18:06:12,899 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 18:07:52,946 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 18:09:32,143 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 18:11:12,006 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 18:12:53,797 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 18:14:33,260 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 18:16:16,906 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 18:17:56,656 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 18:19:36,026 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 18:21:17,122 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 18:22:56,587 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 18:24:37,345 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 18:26:13,646 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 18:27:53,889 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 18:29:35,556 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 18:31:15,331 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 18:32:55,277 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 18:34:34,434 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 18:36:16,659 - INFO - [38000/39832 (95%)]\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/parse/dependencygraph.py:376: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  warnings.warn(\n",
      "2023-06-21 18:38:16,336 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 18:38:16,336 - INFO - dev results:\n",
      "2023-06-21 18:38:16,337 - INFO - Metric              |Score |\n",
      "2023-06-21 18:38:16,337 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 18:38:16,337 - INFO - las                 | 90.57|\n",
      "2023-06-21 18:38:16,337 - INFO - uas                 | 92.50|\n",
      "2023-06-21 18:38:16,472 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 18:38:16,472 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 18:38:16,472 - INFO - Train epoch: 2\n",
      "2023-06-21 18:38:16,511 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 18:39:55,479 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 18:41:34,313 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 18:43:11,073 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 18:44:50,152 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 18:46:26,210 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 18:48:06,147 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 18:49:42,044 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 18:51:19,650 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 18:53:00,584 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 18:54:37,243 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 18:56:15,023 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 18:57:52,953 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 18:59:31,501 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 19:01:05,642 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 19:02:43,894 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 19:04:24,400 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 19:06:01,061 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 19:07:40,594 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 19:09:18,255 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 19:11:16,341 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:11:16,342 - INFO - dev results:\n",
      "2023-06-21 19:11:16,342 - INFO - Metric              |Score |\n",
      "2023-06-21 19:11:16,342 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:11:16,342 - INFO - las                 | 91.94|\n",
      "2023-06-21 19:11:16,342 - INFO - uas                 | 93.55|\n",
      "2023-06-21 19:11:16,482 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 19:11:16,485 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:11:16,486 - INFO - Train epoch: 3\n",
      "2023-06-21 19:11:16,555 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 19:12:53,756 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 19:14:29,636 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 19:16:10,460 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 19:17:47,046 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 19:19:27,818 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 19:21:03,590 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 19:22:40,837 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 19:24:15,203 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 19:25:50,669 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 19:27:27,730 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 19:29:05,153 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 19:30:42,222 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 19:32:19,356 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 19:34:02,747 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 19:35:43,140 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 19:37:22,988 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 19:39:04,707 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 19:40:44,455 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 19:42:24,931 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 19:44:27,800 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:44:27,801 - INFO - dev results:\n",
      "2023-06-21 19:44:27,801 - INFO - Metric              |Score |\n",
      "2023-06-21 19:44:27,801 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:44:27,801 - INFO - las                 | 91.98|\n",
      "2023-06-21 19:44:27,801 - INFO - uas                 | 93.64|\n",
      "2023-06-21 19:44:27,942 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-21 19:44:27,945 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 19:44:27,946 - INFO - Train epoch: 4\n",
      "2023-06-21 19:44:27,994 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 19:46:05,044 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 19:47:45,307 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 19:49:23,292 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 19:51:02,194 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 19:52:42,124 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 19:54:22,724 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 19:56:06,689 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 19:57:51,376 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 19:59:33,965 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 20:01:18,510 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 20:03:04,235 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 20:04:46,571 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 20:06:29,491 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 20:08:39,298 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 20:10:44,736 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 20:12:33,756 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 20:14:57,426 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 20:16:42,354 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 20:18:22,867 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 20:20:29,356 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:20:29,358 - INFO - dev results:\n",
      "2023-06-21 20:20:29,358 - INFO - Metric              |Score |\n",
      "2023-06-21 20:20:29,359 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:20:29,359 - INFO - las                 | 91.91|\n",
      "2023-06-21 20:20:29,359 - INFO - uas                 | 93.57|\n",
      "2023-06-21 20:20:29,568 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:20:29,568 - INFO - Train epoch: 5\n",
      "2023-06-21 20:20:29,641 - INFO - [0/39832 (0%)]\n",
      "2023-06-21 20:22:07,846 - INFO - [2000/39832 (5%)]\n",
      "2023-06-21 20:23:47,710 - INFO - [4000/39832 (10%)]\n",
      "2023-06-21 20:25:28,536 - INFO - [6000/39832 (15%)]\n",
      "2023-06-21 20:27:06,352 - INFO - [8000/39832 (20%)]\n",
      "2023-06-21 20:28:42,476 - INFO - [10000/39832 (25%)]\n",
      "2023-06-21 20:30:19,461 - INFO - [12000/39832 (30%)]\n",
      "2023-06-21 20:31:58,066 - INFO - [14000/39832 (35%)]\n",
      "2023-06-21 20:33:35,650 - INFO - [16000/39832 (40%)]\n",
      "2023-06-21 20:35:16,904 - INFO - [18000/39832 (45%)]\n",
      "2023-06-21 20:36:57,226 - INFO - [20000/39832 (50%)]\n",
      "2023-06-21 20:38:36,750 - INFO - [22000/39832 (55%)]\n",
      "2023-06-21 20:40:15,498 - INFO - [24000/39832 (60%)]\n",
      "2023-06-21 20:41:55,809 - INFO - [26000/39832 (65%)]\n",
      "2023-06-21 20:43:32,222 - INFO - [28000/39832 (70%)]\n",
      "2023-06-21 20:45:12,180 - INFO - [30000/39832 (75%)]\n",
      "2023-06-21 20:46:49,751 - INFO - [32000/39832 (80%)]\n",
      "2023-06-21 20:48:29,767 - INFO - [34000/39832 (85%)]\n",
      "2023-06-21 20:50:08,966 - INFO - [36000/39832 (90%)]\n",
      "2023-06-21 20:51:48,094 - INFO - [38000/39832 (95%)]\n",
      "2023-06-21 20:53:49,198 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:53:49,198 - INFO - dev results:\n",
      "2023-06-21 20:53:49,198 - INFO - Metric              |Score |\n",
      "2023-06-21 20:53:49,199 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 20:53:49,199 - INFO - las                 | 91.85|\n",
      "2023-06-21 20:53:49,199 - INFO - uas                 | 93.45|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1 --activation_function relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTZmqi6eEjO-",
    "outputId": "a534bb50-b21f-4db2-bd23-163c82988f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 21:07:24,123 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=False_activation=relu_encoder=lstm_date=06_21_2023', 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 1, 'no_cuda': False, 'log_interval': 2000, 'do_eval': True, 'pretrained_emb': None, 'activation_function': 'relu', 'encoder': 'lstm', 'experiment_dir': 'results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=False_activation=relu_encoder=lstm_date=06_21_2023'}\n",
      "2023-06-21 21:07:24,173 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "2023-06-21 21:08:10,535 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 21:08:10,536 - INFO - test results:\n",
      "2023-06-21 21:08:10,536 - INFO - Metric              |Score |\n",
      "2023-06-21 21:08:10,536 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-21 21:08:10,536 - INFO - las                 | 92.16|\n",
      "2023-06-21 21:08:10,536 - INFO - uas                 | 93.66|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n_lstm_layers 1 --activation_function relu --do_eval --model_dir=\"results/ds=ptb_epochs=5_lr=0.001_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=1_pretrained_emb=False_activation=relu_encoder=lstm_date=06_21_2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_iTyGR8_emQ"
   },
   "source": [
    "### 4.\n",
    "To implement the BERT based parser, we created a new file `model_bert.py` with a `BertBISTParser` inside. In the constructor we create a `BertTokenizerFast` and a `BertModel`, both based on the `bert-base-uncased` version of BERT.\n",
    "\n",
    "```\n",
    "# BERT initialization\n",
    "self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "self.bert_hid_dim = self.encoder.config.hidden_size\n",
    "```\n",
    "\n",
    "We also modify the MLPs so that their input size matches the hidden layer size of BERT.\n",
    "```\n",
    "encodings = self.tokenizer([w[0] for w in sentence], truncation=True, padding='max_length', is_split_into_words=True)\n",
    "```\n",
    "In the `forward` method, we tokenized the (already split into words) sentence, using the bert tokenizer. At this point, we also keep the positions of the first sub-words of each word, as follows:\n",
    "```\n",
    "word_ids = encodings.word_ids()\n",
    "first_subword_inds = []\n",
    "seen_ids = set()\n",
    "for i, wid in enumerate(word_ids):\n",
    "    if wid is None:\n",
    "        continue\n",
    "    if wid not in seen_ids:\n",
    "        first_subword_inds.append(i)\n",
    "    seen_ids.add(wid)\n",
    "first_subword_inds = torch.LongTensor(first_subword_inds).to(self.device)\n",
    "```\n",
    "\n",
    "After passing the encoded sentence through bert, we keep the last hidden states of BERT that correspond to the first sub-words of each word:\n",
    "```\n",
    "hidden_vectors = hidden_vectors[:, first_subword_inds, :]\n",
    "```\n",
    "Finally, these hidden states pass through the classification MLPs.\n",
    "\n",
    "To enable the bert model, we added an argument `--encoder bert` in `main.py`. We also set the learning rate to 1e-5 since the default one did not provide good results.\n",
    "\n",
    "Note: due to limitations in Google colab GPU availability, the BERT-based model\n",
    "was trained for 2 epochs.\n",
    "\n",
    "Commands:\n",
    "```\n",
    "python main.py --encoder bert --lr 1e-5\n",
    "python main.py --encoder bert --do_eval --model_dir=\"results/ds=ptb_epochs=5_lr=1e-05_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=2_pretrained_emb=False_activation=tanh_encoder=bert_date=06_22_2023\"\n",
    "```\n",
    "\n",
    "The model's performance in the test set is:\n",
    "\n",
    "UAS: 96.19\n",
    "\n",
    "LAS: 94.52\n",
    "\n",
    "The Graph-based dependency parser with BERT encoder outperforms both the models of part A and the LSTM based models of Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjAjTxWz_fnz",
    "outputId": "e542736c-8e49-4271-e1cb-7309d239300b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 18:27:08,990 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': None, 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 1e-05, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 2, 'no_cuda': False, 'log_interval': 2000, 'do_eval': False, 'pretrained_emb': None, 'activation_function': 'tanh', 'encoder': 'bert', 'experiment_dir': './results/ds=ptb_epochs=5_lr=1e-05_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=2_pretrained_emb=False_activation=tanh_encoder=bert_date=06_22_2023'}\n",
      "2023-06-22 18:27:10,481 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-06-22 18:27:25,357 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 18:27:25,357 - INFO - Train epoch: 1\n",
      "2023-06-22 18:27:25,917 - INFO - [0/39832 (0%)]\n",
      "2023-06-22 18:32:50,963 - INFO - [2000/39832 (5%)]\n",
      "2023-06-22 18:38:12,530 - INFO - [4000/39832 (10%)]\n",
      "2023-06-22 18:43:26,925 - INFO - [6000/39832 (15%)]\n",
      "2023-06-22 18:48:38,134 - INFO - [8000/39832 (20%)]\n",
      "2023-06-22 18:53:39,746 - INFO - [10000/39832 (25%)]\n",
      "2023-06-22 18:58:40,547 - INFO - [12000/39832 (30%)]\n",
      "2023-06-22 19:03:42,627 - INFO - [14000/39832 (35%)]\n",
      "2023-06-22 19:08:41,636 - INFO - [16000/39832 (40%)]\n",
      "2023-06-22 19:13:36,985 - INFO - [18000/39832 (45%)]\n",
      "2023-06-22 19:18:33,112 - INFO - [20000/39832 (50%)]\n",
      "2023-06-22 19:23:25,126 - INFO - [22000/39832 (55%)]\n",
      "2023-06-22 19:28:12,813 - INFO - [24000/39832 (60%)]\n",
      "2023-06-22 19:32:59,141 - INFO - [26000/39832 (65%)]\n",
      "2023-06-22 19:37:49,285 - INFO - [28000/39832 (70%)]\n",
      "2023-06-22 19:42:39,446 - INFO - [30000/39832 (75%)]\n",
      "2023-06-22 19:47:28,499 - INFO - [32000/39832 (80%)]\n",
      "2023-06-22 19:52:14,320 - INFO - [34000/39832 (85%)]\n",
      "2023-06-22 19:57:01,613 - INFO - [36000/39832 (90%)]\n",
      "2023-06-22 20:01:53,780 - INFO - [38000/39832 (95%)]\n",
      "2023-06-22 20:07:31,711 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 20:07:31,711 - INFO - dev results:\n",
      "2023-06-22 20:07:31,711 - INFO - Metric              |Score |\n",
      "2023-06-22 20:07:31,712 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 20:07:31,712 - INFO - las                 | 93.29|\n",
      "2023-06-22 20:07:31,712 - INFO - uas                 | 95.44|\n",
      "2023-06-22 20:07:34,151 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-22 20:07:34,155 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 20:07:34,156 - INFO - Train epoch: 2\n",
      "2023-06-22 20:07:34,228 - INFO - [0/39832 (0%)]\n",
      "2023-06-22 20:12:13,632 - INFO - [2000/39832 (5%)]\n",
      "2023-06-22 20:16:50,688 - INFO - [4000/39832 (10%)]\n",
      "2023-06-22 20:21:26,351 - INFO - [6000/39832 (15%)]\n",
      "2023-06-22 20:26:08,151 - INFO - [8000/39832 (20%)]\n",
      "2023-06-22 20:30:47,491 - INFO - [10000/39832 (25%)]\n",
      "2023-06-22 20:35:24,767 - INFO - [12000/39832 (30%)]\n",
      "2023-06-22 20:40:05,680 - INFO - [14000/39832 (35%)]\n",
      "2023-06-22 20:44:43,945 - INFO - [16000/39832 (40%)]\n",
      "2023-06-22 20:49:19,382 - INFO - [18000/39832 (45%)]\n",
      "2023-06-22 20:53:58,516 - INFO - [20000/39832 (50%)]\n",
      "2023-06-22 20:58:35,023 - INFO - [22000/39832 (55%)]\n",
      "2023-06-22 21:03:11,921 - INFO - [24000/39832 (60%)]\n",
      "2023-06-22 21:07:47,988 - INFO - [26000/39832 (65%)]\n",
      "2023-06-22 21:12:25,083 - INFO - [28000/39832 (70%)]\n",
      "2023-06-22 21:17:05,280 - INFO - [30000/39832 (75%)]\n",
      "2023-06-22 21:21:40,664 - INFO - [32000/39832 (80%)]\n",
      "2023-06-22 21:26:19,580 - INFO - [34000/39832 (85%)]\n",
      "2023-06-22 21:31:00,849 - INFO - [36000/39832 (90%)]\n",
      "2023-06-22 21:35:36,407 - INFO - [38000/39832 (95%)]\n",
      "2023-06-22 21:41:08,040 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 21:41:08,040 - INFO - dev results:\n",
      "2023-06-22 21:41:08,040 - INFO - Metric              |Score |\n",
      "2023-06-22 21:41:08,041 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 21:41:08,041 - INFO - las                 | 93.89|\n",
      "2023-06-22 21:41:08,041 - INFO - uas                 | 95.80|\n",
      "2023-06-22 21:41:09,402 - INFO - ---UAS was improved. The current parser was saved.---\n",
      "2023-06-22 21:41:09,405 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-22 21:41:09,406 - INFO - Train epoch: 3\n",
      "2023-06-22 21:41:09,483 - INFO - [0/39832 (0%)]\n"
     ]
    }
   ],
   "source": [
    "!python main.py --encoder bert --lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2Qb1rzLZcvk",
    "outputId": "8e941313-c89f-4d2e-a785-2d8ed9132cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-23 18:54:32,890 - INFO - Experiment Parameters - \n",
      "{'train_path': 'data/train.conll', 'dev_path': 'data/dev.conll', 'test_path': 'data/test.conll', 'ds_name': 'ptb', 'model_dir': 'results/ds=ptb_epochs=5_lr=1e-05_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=2_pretrained_emb=False_activation=tanh_encoder=bert_date=06_22_2023', 'ext_emb': None, 'seed': 1234, 'epochs': 5, 'lr': 0.001, 'alpha': 0.25, 'w_emb_dim': 100, 'pos_emb_dim': 25, 'lstm_hid_dim': 125, 'mlp_hid_dim': 100, 'n_lstm_layers': 2, 'no_cuda': False, 'log_interval': 2000, 'do_eval': True, 'pretrained_emb': None, 'activation_function': 'tanh', 'encoder': 'bert', 'experiment_dir': 'results/ds=ptb_epochs=5_lr=1e-05_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=2_pretrained_emb=False_activation=tanh_encoder=bert_date=06_22_2023'}\n",
      "2023-06-23 18:54:33,598 - INFO - Vocab statistics: words - 34327 | relations - 40 | POS tags - 19\n",
      "Downloading (â€¦)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 158kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 232kB [00:00, 10.9MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 466kB [00:00, 8.58MB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.07MB/s]\n",
      "Downloading model.safetensors: 100% 440M/440M [00:01<00:00, 221MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-06-23 18:56:43,379 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-23 18:56:43,380 - INFO - test results:\n",
      "2023-06-23 18:56:43,380 - INFO - Metric              |Score |\n",
      "2023-06-23 18:56:43,380 - INFO - -----------+-----------+-----------+-----------+-----------\n",
      "2023-06-23 18:56:43,380 - INFO - las                 | 94.52|\n",
      "2023-06-23 18:56:43,381 - INFO - uas                 | 96.19|\n"
     ]
    }
   ],
   "source": [
    "!python main.py --encoder bert --do_eval --model_dir=\"results/ds=ptb_epochs=5_lr=1e-05_seed=1234_extEmb=False_wDim=100_pDim=25_lstmDim=125_mlpDim=100_lstmN=2_pretrained_emb=False_activation=tanh_encoder=bert_date=06_22_2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dEZVU4O_gCf"
   },
   "source": [
    "### 5.\n",
    "\n",
    "\n",
    "We observe that the graph-based dependency parsers of part B outperformed the transition-based dependency parsers of part A, in terms of UAS. Regarding the transition-based models, the initial model achieved the best performance (UAS: 89.19). Adding one extra layer in the MLP provided the same results, whereas the other changes performed worse. Regarding the graph-based model with LSTM encoder, the use of ReLU instead of tanh provided a small improvement over the model of B1 (UAS: 93.66 vs. 93.56), and using pre-trained embeddings instead of randomly initialized provided an even larger but still relatively small improvement over B1(UAS: 93.71 vs. 93.56). Finally, changing the LSTM encoder with the BERT encoder, provided the largest impact in performance, by achieving UAS: 96.19, vs. 93.71 which is the next best performance.\n",
    "\n",
    "\n",
    "The Transition based parsers aim to predict a sequence of actions (transitions), based on which a dependency tree can be produced. They have relatively simple complexity and thus are efficient, but they are prone to error propagation since an early wrong prediction in this sentence can affect the subsequent predictions.\n",
    "\n",
    "\n",
    "On the other hand, the Graph-based parsers try to learn a dependency tree scoring function and detect the highest scoring dependency tree for a sentence. They analyze the whole sentence and thus can extract more complex dependencies. However, this makes the parser more complex and thus slower than the Transition based parser."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
